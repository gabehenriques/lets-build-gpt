{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44f2c98",
   "metadata": {},
   "source": [
    "# Let's Build a GPT\n",
    "\n",
    "\n",
    "This is a companion notebook to [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) video by Andrej Karpathy.\n",
    "\n",
    "This notebook also provides an in-depth introduction to LLMs. You can run this notebook locally, on [Colab](https://colab.research.google.com/), or on your preferred cloud service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75ce42",
   "metadata": {},
   "source": [
    "\n",
    "## Goal: Make a computer program that writes like Shakespeare\n",
    "\n",
    "Dataset: [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt). This dataset is a .txt file comprising the collective works of Shakespeare.\n",
    "\n",
    "Our goal is to write a program that predicts the sequence of characters that mimics Shakespeare's style. Given a sequence of characters, the transformer within the neural network will predict the next most likely character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aab538",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We're going to read the file `dataset/shakespeare.txt`` that will serve as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9420626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/tinyshakespeare.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "len(content)\n",
    "\n",
    "print(content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf15c9",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing Character Vocabulary\n",
    "\n",
    "When training character-based models, each character serves as a \"token\" (the smallest unit the model deals with). We need to know all possible tokens to convert characters to integers (and vice versa).\n",
    "\n",
    "\n",
    "This code identifies and counts all unique characters in the text. This is essential for training a GPT so it can recognize and predict each possible character in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5cc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# text is a series of characters in python\n",
    "chars = sorted(list(set(content)))  # gets all unique characters in the dataset sorted\n",
    "vocab_size = len(chars)  # possible elements of the sequence\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b754a1",
   "metadata": {},
   "source": [
    "### Tokenizing characters\n",
    "\n",
    "Large language models such as GPT, LLaMA, and PaLM work in terms of tokens. They take text, convert it into tokens (integers), then predict which tokens should come next.\n",
    "\n",
    "So, **tokenization** is the process of breaking down a piece of text into tokens that a model can understand. By understanding the statistical relationships between these tokens, models can predict the next token in a sequence of tokens.\n",
    "\n",
    "A few well known tokenizers:\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "- [openai/tiktoken](https://github.com/openai/tiktoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4655134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42]\n",
      "hello, world\n"
     ]
    }
   ],
   "source": [
    "# this code creates a character-level tokenizer. i.e. converts raw text to a sequence of integers\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_idx[c] for c in s]  # encoder: takes a string, outputs a list of integers\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])  # decoder: takes a list of integers, outputs a string\n",
    "\n",
    "print(encode('hello, world'))\n",
    "print(decode(encode('hello, world')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffcdfd",
   "metadata": {},
   "source": [
    "### Storing the tokens\n",
    "\n",
    "\n",
    "First, we created a tokenizer to convert text into a sequence of integers (tokens). Now, rather than putting these integers in a regular python list, we're putting them in something called a [Tensor](https://pytorch.org/docs/stable/tensors.html). A Tensor is basically the same as a [numpy ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html). It's a generic n-dimensional array that can be used for arbitrary numeric computations.\n",
    "\n",
    "Think of this tensor as a special python list that our computer model likes more because it can run operations a lot faster and more efficiently.\n",
    "\n",
    "At the simplest level:\n",
    "\n",
    "- A `0-dimensional tensor` is just a number (also called a scalar).\n",
    "- A `1-dimensional tensor` is similar to a list of numbers.\n",
    "- A `2-dimensional tensor` is similar to a table (or matrix) of numbers.\n",
    "- A `3-dimensional tensor` can be visualized as a cube of numbers.\n",
    "and so on for higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0313df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# now let's encode the entire dataset and store it into a tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "\n",
    "# by definition, a Tensor is a multi-dimensional matrix that contains elements of a single data type.\n",
    "\n",
    "# encode text it ints, and then store the ints in something called a tensor.\n",
    "# a tensor can be a 1D box (like a line), a 2D box (like a grid), a 3D box (like a cube), or more.\n",
    "data = torch.tensor(encode(content), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # this shows what the first 100 chars in the dataset look like to a GPT.\n",
    "\n",
    "# the entire dataset is now represented as a tensor of shape (numb_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4642f",
   "metadata": {},
   "source": [
    "### Monitoring Overfitting: splitting the dataset into training and validation sets\n",
    "\n",
    "By dividing our data into training and validation sets, we can gauge if our model is overfitting. Overfitting happens when the model gets too tuned to the training data and struggles with new, unseen data.\n",
    "\n",
    "Our goal isn't for the neural network to merely memorize Shakespeare's works. Instead, we want it to generate fresh new text that still feels Shakespearean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into training and validation sets\n",
    "\n",
    "n = int(0.9 * len(data))  # 90% of the data for training\n",
    "train_data = data[:n] # first 90% of the data for training\n",
    "val_data = data[n:] # last 10% of the data for validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8dabc",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Next, we'll feed the sequence of tokens into the GPT model. By training it, the model will pick up on patterns and learn to anticipate the following token in the sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
