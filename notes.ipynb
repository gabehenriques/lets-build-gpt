{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44f2c98",
   "metadata": {},
   "source": [
    "# Let's Build a GPT\n",
    "\n",
    "\n",
    "This is a companion notebook to [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) video by Andrej Karpathy.\n",
    "\n",
    "This notebook also provides an in-depth introduction to LLMs. You can run this notebook locally, on [Colab](https://colab.research.google.com/), or on your preferred cloud service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75ce42",
   "metadata": {},
   "source": [
    "\n",
    "## Goal: Make a computer program that writes like Shakespeare\n",
    "\n",
    "Dataset: [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt). This dataset is a .txt file comprising the collective works of Shakespeare.\n",
    "\n",
    "Our goal is to write a program that predicts the sequence of characters that mimics Shakespeare's style. Given a sequence of characters, the transformer within the neural network will predict the next most likely character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aab538",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We're going to read the file `dataset/shakespeare.txt`` that will serve as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9420626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/tinyshakespeare.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "len(content)\n",
    "\n",
    "print(content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf15c9",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing Character Vocabulary\n",
    "\n",
    "When training character-based models, each character serves as a \"token\" (the smallest unit the model deals with). We need to know all possible tokens to convert characters to integers (and vice versa).\n",
    "\n",
    "\n",
    "This code identifies and counts all unique characters in the text. This is essential for training a GPT so it can recognize and predict each possible character in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc5cc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# text is a series of characters in python\n",
    "chars = sorted(list(set(content)))  # gets all unique characters in the dataset sorted\n",
    "vocab_size = len(chars)  # possible elements of the sequence\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b754a1",
   "metadata": {},
   "source": [
    "### Tokenizing characters\n",
    "\n",
    "Large language models such as GPT, LLaMA, and PaLM work in terms of tokens. They take text, convert it into tokens (integers), then predict which tokens should come next.\n",
    "\n",
    "So, **tokenization** is the process of breaking down a piece of text into tokens that a model can understand. By understanding the statistical relationships between these tokens, models can predict the next token in a sequence of tokens.\n",
    "\n",
    "A few well known tokenizers:\n",
    "\n",
    "- [google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "- [openai/tiktoken](https://github.com/openai/tiktoken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4655134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42]\n",
      "hello, world\n"
     ]
    }
   ],
   "source": [
    "# this code creates a character-level tokenizer. i.e. converts raw text to a sequence of integers\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [char_to_idx[c] for c in s]  # encoder: takes a string, outputs a list of integers\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])  # decoder: takes a list of integers, outputs a string\n",
    "\n",
    "print(encode('hello, world'))\n",
    "print(decode(encode('hello, world')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffcdfd",
   "metadata": {},
   "source": [
    "### Storing the tokens\n",
    "\n",
    "\n",
    "First, we created a tokenizer to convert text into a sequence of integers (tokens). Now, rather than putting these integers in a regular python list, we're putting them in something called a [Tensor](https://pytorch.org/docs/stable/tensors.html). A Tensor is basically the same as a [numpy ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html). It's a generic n-dimensional array that can be used for arbitrary numeric computations.\n",
    "\n",
    "Think of this tensor as a special python list that our computer model likes more because it can run operations a lot faster and more efficiently.\n",
    "\n",
    "At the simplest level:\n",
    "\n",
    "- A `0-dimensional tensor` is just a number (also called a scalar).\n",
    "- A `1-dimensional tensor` is similar to a list of numbers.\n",
    "- A `2-dimensional tensor` is similar to a table (or matrix) of numbers.\n",
    "- A `3-dimensional tensor` can be visualized as a cube of numbers.\n",
    "and so on for higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0313df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# now let's encode the entire dataset and store it into a tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "\n",
    "# by definition, a Tensor is a multi-dimensional matrix that contains elements of a single data type.\n",
    "\n",
    "# encode text it ints, and then store the ints in something called a tensor.\n",
    "# a tensor can be a 1D box (like a line), a 2D box (like a grid), a 3D box (like a cube), or more.\n",
    "data = torch.tensor(encode(content), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # this shows what the first 100 chars in the dataset look like to a GPT.\n",
    "\n",
    "# the entire dataset is now represented as a tensor of shape (numb_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4642f",
   "metadata": {},
   "source": [
    "### Monitoring Overfitting: splitting the dataset into training and validation sets\n",
    "\n",
    "By dividing our data into training and validation sets, we can gauge if our model is overfitting. Overfitting happens when the model gets too tuned to the training data and struggles with new, unseen data.\n",
    "\n",
    "Our goal isn't for the neural network to merely memorize Shakespeare's works. Instead, we want it to generate fresh new text that still feels Shakespearean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b28d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into training and validation sets\n",
    "\n",
    "n = int(0.9 * len(data))  # 90% of the data for training\n",
    "train_data = data[:n] # first 90% of the data for training\n",
    "val_data = data[n:] # last 10% of the data for validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8dabc",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Next, we'll feed the sequence of tokens into the GPT model. By training it, the model will pick up on patterns and learn to anticipate the following token in the sequence.\n",
    "\n",
    "\n",
    "https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=YJb0OXPwzvqg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d5ab7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 48, 57, 58, 59,  2, 16, 48])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "block_size = 8\n",
    "train_data[:block_size]+1 # we're gonna train it in every single position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280361ea",
   "metadata": {},
   "source": [
    "\n",
    "#### Sequence prediction using the \"sliding window\" technique\n",
    "\n",
    "At a high level, the following code is trying to train a model to predict the next item in a sequence based on the items that came before it.\n",
    "\n",
    "The main idea is to teach the model to predict the next item in a sequence based on a given context from that sequence. The training data is prepared using a \"sliding window\" approach, which, in this example, starts with a single token and expands to include more tokens from the sequence as context.\n",
    "\n",
    "Imagine a series of tokens as a horizontal line of blocks, like this:\n",
    "\n",
    "```python\n",
    "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "```\n",
    "\n",
    "For block_size of 8, the sliding window approach will start with a small window over the first token and gradually increase in size, like so:\n",
    "\n",
    "```python\n",
    "tensor([1]) # next token (or target): tensor(2)\n",
    "tensor([1, 2]) # next token: tensor(3)\n",
    "tensor([1, 2, 3]) # next token: tensor(4)\n",
    "tensor([1, 2, 3, 4]) # next token: tensor(5)\n",
    "tensor([1, 2, 3, 4, 5]) # next token: tensor(6)\n",
    "tensor([1, 2, 3, 4, 5, 6]) # next token: tensor(7)\n",
    "tensor([1, 2, 3, 4, 5, 6, 7]) # next token: tensor(8)\n",
    "tensor([1, 2, 3, 4, 5, 6, 7, 8]) # no next token or target since we're at the end of the tensor.\n",
    "\n",
    "```\n",
    "\n",
    "Parameters:\n",
    "- `block_size`: The length of sequences to consider. For instance, if `block_size` is 8, sequences of length up to `8` are processed.\n",
    "- `train_data`: A Tensor of tokens representing the data used for training.\n",
    "\n",
    "Variables:\n",
    "- `x`: The initial context, which starts from the first token and includes up to `block_size` tokens.\n",
    "- `y`: The corresponding targets for each context in `x`. Each item in `y` is the next token after the corresponding context in `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834ef84",
   "metadata": {},
   "source": [
    "### Analogy: teaching a kid to guess the next word(s) in a story\n",
    "\n",
    "\n",
    "Think of this model training process like teaching a kid to guess the next word in a story.\n",
    "\n",
    "\n",
    "- **Different story lengths (using the sliding window technique)**\n",
    "Sometimes we tell them just one word from the story, sometimes two words, sometimes three, and so on, up to a certain limit (like eight words). This is like giving the model different lengths of 'stories' to learn from.\n",
    "\n",
    "- **Why vary lengths?**\n",
    "This helps the kid (or our model) get comfortable with predicting the next word whether they've heard a short bit of the story or a longer bit. It's like practicing with different levels of difficulty.\n",
    "\n",
    "- **Practice**\n",
    "Once the kid has practiced enough, you can give them any short or long part of the story, and they'll try to guess the next word. This is because they've practiced with all different story lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fa0f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when the context is tensor([18]) the target is tensor(47)\n",
      "when the context is tensor([18, 47]) the target is tensor(56)\n",
      "when the context is tensor([18, 47, 56]) the target is tensor(57)\n",
      "when the context is tensor([18, 47, 56, 57]) the target is tensor(58)\n",
      "when the context is tensor([18, 47, 56, 57, 58]) the target is tensor(1)\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1]) the target is tensor(15)\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15]) the target is tensor(47)\n",
      "when the context is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is tensor(58)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "# think of `x`` as the question and y as the answer. For example, if x is \"how are\" then y is \"are you?\"\n",
    "print(train_data[:block_size+1])\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(\"when the context is\", context, \"the target is\",target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23c5c7",
   "metadata": {},
   "source": [
    "\n",
    "In this example, we change how much information we give the model by varying the length of context, from just 1 token up to `block_size`. We do this so:\n",
    "\n",
    "1. The model can be good at predicting with both short and long bits of information i.e. it becomes adept at generating predictions with any context length.\n",
    "2. It understands how pieces of information connect in different situations. The model learns to anticipate and predict subsequent tokens based on different lengths of historical context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
